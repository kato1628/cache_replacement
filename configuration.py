config = {
    "experiment": {
        "base_dir": "./experiments",
        "name": "test",
    },
    "dataset": {
        "filepath": "./dataset/wiki2018_dev_two_million.tr",
        "window_size": 2200000,
        # "capacity": 2000000000,
        "capacity": 4000000000,
        "access_history_len": 1,
        "scorer_type": "mixture"
    },
    "model": {
        "obj_id_embedder": {
            "type": "dynamic_vocab",
            # "max_vocab_size": 10000,
            "max_vocab_size": 20000,
            # "embedding_dim": 64,
            "embedding_dim": 64,
        },
        "obj_size_embedder": {
            "type": "logarithmic",
            # "embedding_dim": 64,
            "embedding_dim": 12,
            "max_size": 1000000000,
            "max_vocab_size": 20,
        },
        "cache_lines_embedder": "obj_id_embedder",
        "positional_embedder": {
            "type": "positional",
            # "embedding_dim": 128,
            "embedding_dim": 64,
        },
        # "lstm_hidden_size": 128,
        "lstm_hidden_size": 64,
        "max_attention_history": 40,
    },
    "dagger_schedule" : {
        "type": "linear",
        "initial": 0.0,
        "final": 1.0,
        # "num_steps": 20000,
        "num_steps": 1000,
        # "update_frequency": 10000,
        "update_frequency": 10,
    },
    "training": {
        "learning_rate": 0.001,
        # "batch_size": 32,
        "batch_size": 4,
        # "sequence_length": 80,
        "sequence_length": 20,
        # "collection_multiplier": 5,
        "collection_multiplier": 5,
        # "total_steps": 1000000,
        "total_steps": 2000,
        # "save_frequency": 20000,
        "save_frequency": 200,
        # "evaluation_frequency": 400,
        "evaluation_frequency": 40,
        # "evaluation_size": 30000,
        "log_loss_frequency": 10,
        "evaluation_size": 50000,
        "checkpoint_dir": "./result/checkpoints",
    },
}